{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6908d5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vinu1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\vinu1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vinu1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Required Libraries\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('twitter_samples')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "stopwords_english = stopwords.words('english')\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dbaba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the Dataframe cell \n",
    "\n",
    "df_test = pd.read_csv('C:\\\\Users\\\\vinu1\\\\Personality_perception\\\\data\\\\Tweets\\\\twitter_data.csv')\n",
    "# df.drop(['Unnamed: 0', 'Index',  \n",
    "#                 'Location', 'Subcategory',\n",
    "#        'Unnamed: 5'], axis=1, inplace=True)\n",
    "# df['text'] = df['text'].str.lower()\n",
    "# df['text']\n",
    "\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "train_pos = all_positive_tweets[:]\n",
    "train_neg = all_negative_tweets[:]\n",
    "train_x = train_pos+train_neg\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "df_train = pd.DataFrame()\n",
    "df_train[\"text\"] = train_x\n",
    "df_train[\"sentiment\"] = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2d9a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanData:\n",
    "    \n",
    "    def __init__(self, df_test,df_train):\n",
    "         self.df_test_cleaned = df_test\n",
    "         self.df_train_cleaned = df_train\n",
    "         self.Openness_list = []\n",
    "         self.Agreeableness_list = []\n",
    "         self.all_positive_tweets = []\n",
    "         self.all_negative_tweets = []\n",
    "         self.stopwords_english = stopwords.words('english')\n",
    "        \n",
    "    def read_persona_dict(self, Personality):\n",
    "        for i in Personality:\n",
    "            path = f\"C:\\\\Users\\\\vinu1\\\\Personality_perception\\\\data\\\\Dictionary\\\\{i}.txt\"\n",
    "            with open(path) as file:\n",
    "                if i == \"Openness\":\n",
    "                   self.Openness_list = [line.rstrip() for line in file]\n",
    "                   self.Openness_list = [re.sub(r\"[^a-zA-Z0-9]\", \"\", file) for file in self.Openness_list]\n",
    "                elif i == \"Agreeableness\":\n",
    "                   self.Agreeableness_list = [line.rstrip() for line in file]\n",
    "                   self.Agreeableness_list = [re.sub(r\"[^a-zA-Z0-9]\", \"\", file) for file in self.Agreeableness_list]\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def pre_processing_dataframe(self,df):\n",
    "#         df[\"text\"] = df[\"text\"].str.lower()\n",
    "#         word_token_list = []\n",
    "#         for i in range(len(df.index)):\n",
    "#             word_token = word_tokenize(df[\"text\"][i])\n",
    "#             word_token_list.append(word_token)\n",
    "#         df1 = pd.DataFrame(word_token_list)\n",
    "#         df1['tokenized_tweets']=df1.apply(lambda row: row.dropna().tolist(), axis=1)\n",
    "#         df_token = df1[['tokenized_tweets']]\n",
    "#         df = pd.concat([df.reset_index(drop=True), df_token], axis=1)\n",
    "        \n",
    "        stopwords_english = stopwords.words('english')\n",
    "        punctuations = string.punctuation\n",
    "        stemmer = PorterStemmer()\n",
    "        #### Removing the unwanted things in the tweets\n",
    "        df[\"text\"]=df[\"text\"].apply(lambda x: re.sub(r'RT[\\s]+','', str(x)))\n",
    "        df['text']=df['text'].apply(lambda x: re.sub(r\"http\\S+\",'', str(x)))\n",
    "        df['text']=df['text'].apply(lambda x: re.sub(r'#', '', str(x)))\n",
    "        df['tokenized_tweets']=df['text'].apply(lambda x: word_tokenize(x))\n",
    "        \n",
    "        df['tokenized_tweets'] = df['tokenized_tweets'].apply(lambda x: [word.lower() for word in x if (word not in stopwords_english and word not in punctuations)])\n",
    "\n",
    "\n",
    "        \n",
    "    def pre_process(self, x):\n",
    "        if x == \"train\":\n",
    "            self.pre_processing_dataframe(self.df_train_cleaned)\n",
    "        elif x == \"test\":\n",
    "            self.pre_processing_dataframe(self.df_test_cleaned)\n",
    "        \n",
    "    def get_persona_scores(self):\n",
    "        max_list = []\n",
    "        persona_list = []\n",
    "        open_count, agree_count = 0 , 0\n",
    "        open_ratio, agree_ratio = 0 , 0\n",
    "\n",
    "        for i in range(len(self.df_test_cleaned.index)):\n",
    "            max_score = 0\n",
    "            open_count, agree_count = 0 , 0\n",
    "            open_ratio, agree_ratio = 0 , 0\n",
    "            for j in range(len(self.df_test_cleaned[\"tokenized_tweets\"][i])):\n",
    "\n",
    "                if (self.df_test_cleaned[\"tokenized_tweets\"][i][j] in self.Openness_list):\n",
    "                    open_count += 1\n",
    "\n",
    "                    open_ratio = open_count/len(self.Openness_list)\n",
    "                if (self.df_test_cleaned[\"tokenized_tweets\"][i][j] in self.Agreeableness_list):\n",
    "                    agree_count +=1\n",
    "                    agree_ratio = agree_count/len(self.Agreeableness_list)\n",
    "            max_score = max(open_ratio,agree_ratio)\n",
    "            max_list.append(max_score)\n",
    "            \n",
    "\n",
    "            if max_score == 0:\n",
    "                x = \"Unknown\"\n",
    "            elif max_score == open_ratio:\n",
    "                x = \"Openness\"\n",
    "            elif max_score == agree_ratio:\n",
    "                x = \"Agreeableness\"\n",
    "\n",
    "            persona_list.append(x)\n",
    "\n",
    "\n",
    "        self.df_test_cleaned[\"personality_score\"] = max_list\n",
    "        self.df_test_cleaned[\"Personality\"] = persona_list\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e695cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_predict_data(CleanData):\n",
    "   def create_frequency(self):\n",
    "        freq_d = {}\n",
    "        for i in range(len(self.df_train_cleaned.index)):\n",
    "            for word in self.df_train_cleaned[\"tokenized_tweets\"][i]:\n",
    "                pair = (word, self.df_train_cleaned[\"sentiment\"][i])\n",
    "                if pair in freq_d:\n",
    "                    freq_d[pair] += 1\n",
    "                else:\n",
    "                    freq_d[pair] = freq_d.get(pair, 1)\n",
    "    \n",
    "        return freq_d\n",
    "    \n",
    "   def train_naive_bayes(self,freqs, train_x, train_y):\n",
    "        loglikelihood = {}\n",
    "        logprior = 0\n",
    "\n",
    "        unique_words = set([pair[0] for pair in freqs.keys()])\n",
    "\n",
    "        V = len(unique_words)\n",
    "\n",
    "        N_pos = N_neg = 0\n",
    "        for pair in freqs.keys():\n",
    "            if pair[1]>0:\n",
    "                N_pos+=freqs[(pair)]\n",
    "\n",
    "            else:\n",
    "                N_neg += freqs[(pair)]\n",
    "        D = train_y.shape[0]\n",
    "\n",
    "        D_pos = sum(train_y)\n",
    "\n",
    "        D_neg = D-sum(train_y)\n",
    "\n",
    "        logprior = np.log(D_pos) - np.log(D_neg)\n",
    "\n",
    "        for word in unique_words:\n",
    "            freq_pos = freqs.get((word, 1),0)\n",
    "            freq_neg = freqs.get((word, 0), 0)\n",
    "            p_w_pos = (freq_pos+1)/(N_pos+V)\n",
    "            p_w_neg = (freq_neg+1)/(N_neg+V)\n",
    "            loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "        return logprior, loglikelihood\n",
    "    \n",
    "   def naive_bayes_predict(self,logprior,loglikelihood):\n",
    "       sentiment_score = []\n",
    "       for i in range(len(self.df_test_cleaned.index)):\n",
    "               p = 0\n",
    "               p+=logprior\n",
    "               for word in self.df_test_cleaned[\"tokenized_tweets\"][i]:\n",
    "                 if word in loglikelihood:\n",
    "                   p+=loglikelihood[word]\n",
    "               sentiment_score.append(p)\n",
    "       self.df_test_cleaned[\"sentiment_score\"] = sentiment_score\n",
    "    \n",
    "   def sentiment_persona(self):\n",
    "      sentiment_persona = []\n",
    "      for i in range(len(self.df_test_cleaned.index)):\n",
    "                score  = self.df_test_cleaned[\"sentiment_score\"][i]\n",
    "                if (-0.5 < score < 0.5):\n",
    "                    x  = \"Neutral\"\n",
    "                elif (score < -0.5):\n",
    "                    x = \"Negative\"\n",
    "                elif (score > 0.5):\n",
    "                    x = \"Positive\"\n",
    "                sentiment_persona.append(x)\n",
    "      self.df_test_cleaned[\"sentiment_persona\"] = sentiment_persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Personality = ['Openness', 'Agreeableness']\n",
    "# clean_data = CleanData(df)\n",
    "train_data = Training_predict_data(df_test, df_train)\n",
    "train_data.read_persona_dict(Personality)\n",
    "train_data.pre_process(\"test\")\n",
    "train_data.pre_process(\"train\")\n",
    "train_data.get_persona_scores()\n",
    "freq_d=train_data.create_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42683bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior, loglikelihood = train_data.train_naive_bayes(freq_d, train_x, train_y)\n",
    "\n",
    "train_data.naive_bayes_predict(logprior,loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.sentiment_persona()\n",
    "train_data.df_test_cleaned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
